[I] Found 2 configs:
configs/verification/multi_output_bl05_bs2.json
configs/verification/linking.json
[I] Loading config 1/2 configs/verification/multi_output_bl05_bs2.json
[I|Model] Loading Tokenizer
Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]Downloading: 100%|██████████| 29.0/29.0 [00:00<00:00, 179kB/s]
Downloading:   0%|          | 0.00/411 [00:00<?, ?B/s]Downloading: 100%|██████████| 411/411 [00:00<00:00, 4.25MB/s]
Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]Downloading: 100%|██████████| 213k/213k [00:00<00:00, 13.2MB/s]
Downloading:   0%|          | 0.00/436k [00:00<?, ?B/s]Downloading: 100%|██████████| 436k/436k [00:00<00:00, 5.83MB/s]
[I|Model] Loading Model
Downloading:   0%|          | 0.00/263M [00:00<?, ?B/s]Downloading:   3%|▎         | 7.74M/263M [00:00<00:03, 77.4MB/s]Downloading:   7%|▋         | 17.1M/263M [00:00<00:02, 87.2MB/s]Downloading:  10%|█         | 27.4M/263M [00:00<00:02, 94.1MB/s]Downloading:  14%|█▍        | 38.1M/263M [00:00<00:02, 99.3MB/s]Downloading:  19%|█▊        | 49.0M/263M [00:00<00:02, 103MB/s] Downloading:  23%|██▎       | 59.9M/263M [00:00<00:01, 105MB/s]Downloading:  27%|██▋       | 70.8M/263M [00:00<00:01, 106MB/s]Downloading:  31%|███       | 81.8M/263M [00:00<00:01, 107MB/s]Downloading:  35%|███▌      | 92.7M/263M [00:00<00:01, 108MB/s]Downloading:  39%|███▉      | 104M/263M [00:01<00:01, 108MB/s] Downloading:  43%|████▎     | 114M/263M [00:01<00:01, 108MB/s]Downloading:  48%|████▊     | 125M/263M [00:01<00:01, 109MB/s]Downloading:  52%|█████▏    | 136M/263M [00:01<00:01, 109MB/s]Downloading:  56%|█████▌    | 147M/263M [00:01<00:01, 109MB/s]Downloading:  60%|██████    | 158M/263M [00:01<00:00, 109MB/s]Downloading:  64%|██████▍   | 169M/263M [00:01<00:00, 109MB/s]Downloading:  68%|██████▊   | 180M/263M [00:01<00:00, 109MB/s]Downloading:  73%|███████▎  | 191M/263M [00:01<00:00, 109MB/s]Downloading:  77%|███████▋  | 202M/263M [00:01<00:00, 109MB/s]Downloading:  81%|████████  | 213M/263M [00:02<00:00, 109MB/s]Downloading:  85%|████████▍ | 224M/263M [00:02<00:00, 109MB/s]Downloading:  89%|████████▉ | 235M/263M [00:02<00:00, 109MB/s]Downloading:  93%|█████████▎| 246M/263M [00:02<00:00, 109MB/s]Downloading:  97%|█████████▋| 257M/263M [00:02<00:00, 109MB/s]Downloading: 100%|██████████| 263M/263M [00:02<00:00, 107MB/s]
Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[I|Model] Loading tags from file
[I|Model] Loading tag categories from file
/mnt/nas_home/jrb239/miniconda3/envs/tfm_nlp/lib/python3.9/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:115.)
  return torch._C._cuda_getDeviceCount() > 0
Using CPU
[I|Data] Loading data from file
[I|Data] Pre-processing input data
[I|Data] Processing transformer data
[I|Data] Processing tag data
[W] The following tags were not found in the new tag list:
	cell transplantation
[W] The following tags were not found in the new tag list:
	entosis
	netosis
[I|Data] Tag list: {'category': ['CONTEXT', 'EFFECT', 'O', 'PERTURBING_ACTION', 'PAD'], 'description': ['conditional knock in', 'conditional knock out', 'decreased expression level', 'increased expression level', 'knock down', 'knock in', 'knock out', 'other', 'pharmacological inhibition', 'plasmid vector', 'viral vector', 'PAD'], 'experiment_type': ['cell line', 'cell transplantation', 'cells', 'not stated', 'organism', 'organoid culture', 'other', 'primary culture', 'transformed cell line', 'tumour', 'xenotransplantation', 'PAD'], 'species': ['bacterium', 'fish', 'fly', 'human', 'mouse', 'not stated', 'other', 'rat', 'yeast', 'PAD'], 'phenotype': ['adhesion', 'anoikis', 'apoptosis', 'autophagy', 'cell cycle arrest', 'cell death', 'cell growth', 'cell survival', 'colony formation', 'differentiation', 'entosis', 'epithelial-mesenchymal transition', 'ferroptosis', 'invasion', 'metastasis', 'migration', 'mitophagy', 'necroptosis', 'necrosis', 'netosis', 'oncosis', 'proliferation', 'pyroptosis', 'quiescence', 'self-renewal', 'senescence', 'transformation', 'tumour growth', 'tumourigenesis', 'PAD'], 'activity': ['causes', 'decreases', 'increases', 'inhibits', 'no effect', 'not stated', 'regulates', 'PAD']}
[I|Data] Tag weights: {'category': tensor([0.3101, 0.3512, 0.0650, 0.2737]), 'description': tensor([67.9400, 22.1435, 14.2349, 14.8797, 13.3102, 62.7447, 13.1441, 11.9698,
        11.9310, 44.0388, 48.4554]), 'experiment_type': tensor([ 27.3692, 517.4708,   9.3350, 172.4877,  14.1540,  49.3289, 182.9512,
         48.4554,  12.8537,  19.5051,  22.6925]), 'species': tensor([118.7119, 211.2546, 115.7059,  18.0764,  14.4009,   6.9216,  58.2115,
         47.8298, 138.2965]), 'phenotype': tensor([105.6237, 163.6359,  14.9991,  14.5380,  43.4137,  26.2856,  37.5280,
         47.4260, 101.4796,  43.1110, 517.4708,  40.6441,  27.5632,  55.7915,
         28.2127,  39.3299,  44.0388,  54.5371,  57.4882, 517.4708,  97.7878,
         28.2548,  67.9400,  92.9353,  59.3497,  38.7733,  56.4519,  29.2790,
         26.0176]), 'activity': tensor([12.7584, 14.2841, 13.6380, 15.2533, 36.5771, 33.8859, 32.2635])}
[I|Train Handler] Training model
[I|Train] Epoch 0/6
Traceback (most recent call last):
  File "/mnt/nas_home/jrb239/transformer_nlp/main.py", line 90, in <module>
    main(c)
  File "/mnt/nas_home/jrb239/transformer_nlp/main.py", line 32, in main
    mode_funcs[config["MODE"]](config)
  File "/mnt/nas_home/jrb239/transformer_nlp/src/model_handler.py", line 450, in train_mode
    history = self.train(*dataloaders, config=config["TRAINING"])
  File "/mnt/nas_home/jrb239/transformer_nlp/src/model_handler.py", line 122, in train
    return self.model.train_model(train_dataloader, val_dataloader,
  File "/mnt/nas_home/jrb239/transformer_nlp/src/model.py", line 274, in train_model
    outputs = self.forward_batch(batch, class_loss_weights=class_loss_weights)
  File "/mnt/nas_home/jrb239/transformer_nlp/src/model.py", line 211, in forward_batch
    outputs = self(model_data, class_loss_weights=class_loss_weights, calc_loss=calc_loss)
  File "/mnt/nas_home/jrb239/miniconda3/envs/tfm_nlp/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/nas_home/jrb239/transformer_nlp/src/model.py", line 181, in forward
    outputs = {kh: {ki: get_output(vi) for ki, vi in vh.items()}
  File "/mnt/nas_home/jrb239/transformer_nlp/src/model.py", line 182, in <dictcomp>
    if kh != "default" else get_output(vh)
  File "/mnt/nas_home/jrb239/transformer_nlp/src/model.py", line 179, in get_output
    return self.dropout(self.tfm(i_dict["iids"], attention_mask=i_dict["masks"])[0])
  File "/mnt/nas_home/jrb239/miniconda3/envs/tfm_nlp/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/nas_home/jrb239/miniconda3/envs/tfm_nlp/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 481, in forward
    return self.transformer(
  File "/mnt/nas_home/jrb239/miniconda3/envs/tfm_nlp/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/nas_home/jrb239/miniconda3/envs/tfm_nlp/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 306, in forward
    layer_outputs = layer_module(
  File "/mnt/nas_home/jrb239/miniconda3/envs/tfm_nlp/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/nas_home/jrb239/miniconda3/envs/tfm_nlp/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 264, in forward
    ffn_output = self.ffn(sa_output)  # (bs, seq_length, dim)
  File "/mnt/nas_home/jrb239/miniconda3/envs/tfm_nlp/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/nas_home/jrb239/miniconda3/envs/tfm_nlp/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 215, in forward
    return apply_chunking_to_forward(self.ff_chunk, self.chunk_size_feed_forward, self.seq_len_dim, input)
  File "/mnt/nas_home/jrb239/miniconda3/envs/tfm_nlp/lib/python3.9/site-packages/transformers/modeling_utils.py", line 2001, in apply_chunking_to_forward
    return forward_fn(*input_tensors)
  File "/mnt/nas_home/jrb239/miniconda3/envs/tfm_nlp/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 218, in ff_chunk
    x = self.lin1(input)
  File "/mnt/nas_home/jrb239/miniconda3/envs/tfm_nlp/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/nas_home/jrb239/miniconda3/envs/tfm_nlp/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 96, in forward
    return F.linear(input, self.weight, self.bias)
  File "/mnt/nas_home/jrb239/miniconda3/envs/tfm_nlp/lib/python3.9/site-packages/torch/nn/functional.py", line 1847, in linear
    return torch._C._nn.linear(input, weight, bias)
KeyboardInterrupt
